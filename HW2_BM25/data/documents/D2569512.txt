announcement how to read the output from one way analysis of variance heres a typical piece of output from a singlefactor analysis of variance the response is the two year change in bone density of the spine final  initial for postmenopausal women with low daily calcium intakes  400 mg assigned at random to one of three treatmentsplacebo calcium carbonate calcium citrate maleate class levels values group 3 cc ccm p dependent variable dbmd05 sum of source df squares mean square f value pr  f model 2 440070120 220035060 500 00090 error 78 3431110102 43988591 corrected total 80 3871180222 rsquare coeff var root mse dbmd05 mean 0113679 2173832 2097346 0964815 source df type i ss mean square f value pr  f group 2 4400701202 2200350601 500 00090 source df type iii ss mean square f value pr  f group 2 4400701202 2200350601 500 00090 standard parameter estimate error t value pr  t intercept 1520689655 b 038946732 390 00002 group cc 0075889655 b 057239773 013 08949 group ccm 1597356322 b 056089705 285 00056 group p 0000000000 b    note the xx matrix has been found to be singular and a generalized inverse was used to solve the normal equations terms whose estimates are followed by the letter b are not uniquely estimable the glm procedure least squares means dbmd05 lsmean group lsmean number cc 144480000 1 ccm 007666667 2 p 152068966 3 least squares means for effect group pr  t for h0 lsmean ilsmean j ij 1 2 3 1 00107 08949 2 00107 00056 3 08949 00056 note to ensure overall protection level only probabilities associated with preplanned comparisons should be used adjustment for multiple comparisons tukeykramer least squares means for effect group pr  t for h0 lsmean ilsmean j ij 1 2 3 1 00286 09904 2 00286 00154 3 09904 00154the analysis of variance table the analysis of variance table is just like any other anova table the total sum of squares is the uncertainty that would be present if one had to predict individual responses without any other information the best one could do is predict each observation to be equal to the overall sample mean the anova table partitions this variability into two parts one portion is accounted for some say explained by the model its the reduction in uncertainty that occurs when the anova modely ij   i  ij is fitted to the data the remaining portion is the uncertainty that remains even after the model is used the model is considered to be statistically significant if it can account for a large amount of variability in the response model error corrected total sum of squares degrees of freedom f value and pr f have the same meanings as for multiple regression this is to be expected since analysis of variance is nothing more than the regression of the response on a set of indicators definded by the categorical predictor variable the degrees of freedom for the model is equal to one less than the number of categories the f ratio is nothing more than the extra sum of squares principle applied to the full set of indicator variables defined by the categorical predictor variable the f ratio and its p value are the same regardless of the particular set of indicators the constraint placed on the s that is used sums of squares the total amount of variability in the response can be written the sum of the squared differences between each observation and the overall mean if we were asked to make a prediction without any other information the best we can do in a certain sense is the overall mean the amount of variation in the data that cant be accounted for by this simple method of prediction is the total sum of squares when the analysis of variance model is used for prediction the best that can be done is to predict each observation to be equal to its groups mean the amount of uncertainty that remains is sum of the squared differences between each observation and its groups mean this is the error sum of squares in this outpur it also appears as the group sum of squares the difference between the total sum of squares and the error sum of squares is the model sum of squares which happens to be equal to  each sum of squares has corresponding degrees of freedom df associated with it total df is one less than the number of observations n1 the model df is the one less than the number of levels the error df is the difference between the total df n1 and the model df g1 that is ng another way to calculate the error degrees of freedom is by summing up the error degrees of freedom from each group n i 1 over all g groups the mean squares are the sums of squares divided by the corresponding degrees of freedom the f value or f ratio is the test statistic used to decide whether the sample means are withing sampling variability of each other that is it tests the hypothesis h 0 1  g this is the same thing as asking whether the model as a whole has statistically significant predictive capability in the regression framework f is the ratio of the model mean square to the error mean square under the null hypothesis that the model has no predictive capabilitythat is that all of thepopulation means are equalthe f statistic follows an f distribution with p numerator degrees of freedom and np1 denominator degrees of freedom the null hypothesis is rejected if the f ratio is large this statstic and p value might be ignored depending on the primary research question and whether a multiple comparisons procedure is used see the discussion of multiple comparison procedures  the root mean square error also known as the standard error of the estimate is the square root of the residual mean square it estimates the common withingroup standard deviation parameter estimates the parameter estimates from a single factor analysis of variance might best be ignored different statistical program packages fit different paraametrizations of the oneway anova model to the data systat for example uses the usual constraint where i 0 sas on the other hand sets g to 0 any version of the model can be used for prediction but care must be taken with significance tests involving individual terms in the model to make sure they correspond to hypotheses of interest in the sas output above the intercept tests whether the mean bone density in the placebo group is 0 which is after all to be expected while the coefficients for cc and ccm test whether those means are different from placebo it is usually safer to test hypotheses directly by using the whatever facilities the software provides that by taking a chance on the proper interpretation of the model parametrization the software might have implemented the possiblity of many different parametrizations is the subject of the warning that terms whose estimates are followed by the letter b are not uniquely estimable after the parameter estimates come two examples of multiple comparisons procedures which are used to determine which groups are different given that they are not all the same these methods are discussed in detail in the note on multiple comparison procedures the two methods presented here are fishers least significant differences and tukeys honestly signficant differences fishers least significant differences is essentially all possible t tests it differs only in that the estimate of the common within group standard deviation is obtained by pooling information from all of the levels of the factor and not just the two being compared at the moment the values in the matrix of p values comparing groups 13 and 23 are identical to the values for the cc and ccm parameters in the model back to lhsp copyright Â© 2000 gerard e dallal last modified 05232012 025121 
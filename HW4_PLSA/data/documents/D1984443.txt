from wikipedia the free encyclopedianavigation search not to be confused with crosscovariance matrix a bivariate gaussian probability density function centered at 0 0 with covariance matrix  100 050  050 100  sample points from a bivariate gaussian distribution with a standard deviation of 3 in roughly the lower leftupper right direction and of 1 in the orthogonal direction because the x and y components covary the variances of x and y do not fully describe the distribution a 2×2 covariance matrix is needed the directions of the arrows correspond to the eigenvectors of this covariance matrix and their lengths to the square roots of the eigenvalues in probability theory and statistics a covariance matrix also known as dispersion matrix or variance–covariance matrix is a matrix whose element in the i j position is the covariance between the i th and j th elements of a random vector a random vector is a random variable with multiple dimensions each element of the vector is a scalar random variable each element has either a finite number of observed empirical values or a finite or infinite number of potential values the potential values are specified by a theoretical joint probability distribution intuitively the covariance matrix generalizes the notion of variance to multiple dimensions as an example the variation in a collection of random points in twodimensional space cannot be characterized fully by a single number nor would the variances in the x and y directions contain all of the necessary information a 2×2 matrix would be necessary to fully characterize the twodimensional variation because the covariance of the i th random variable with itself is simply that random variables variance each element on the principal diagonal of the covariance matrix is the variance of one of the random variables because the covariance of the i th random variable with the j th one is the same thing as the covariance of the j th random variable with the i th one every covariance matrix is symmetric in addition every covariance matrix is positive semidefinite contents  hide 1 definition11 generalization of the variance12 correlation matrix13 conflicting nomenclatures and notations2 properties21 basic properties22 block matrices3 covariance matrix as a parameter of a distribution4 covariance matrix as a linear operator5 which matrices are covariance matrices6 complex random vectors7 estimation8 applications9 see also10 references11 further reading definition  editthroughout this article boldfaced unsubscripted x and y are used to refer to random vectors and unboldfaced subscripted x and y are used to refer to random scalars if the entries in the column vector are random variables each with finite variance then the covariance matrix σ is the matrix whose  i  j entry is the covariance  where the operator e denotes the expected mean value of its argument andis the expected value of the i th entry in the vector x in other wordse e ee e ee e ethe inverse of this matrix if it exists is the inverse covariance matrix also known as the concentration matrix or precision matrix 1generalization of the variance  editthe definition above is equivalent to the matrix equality this form can be seen as a generalization of the scalarvalued variance to higher dimensions recall that for a scalarvalued random variable xindeed the entries on the diagonal of the covariance matrix are the variances of each element of the vector correlation matrix  edita quantity closely related to the covariance matrix is the correlation matrix the matrix of pearson productmoment correlation coefficients between each of the random variables in the random vector which can be writtenwhere is the matrix of the diagonal elements of ie a diagonal matrix of the variances of for  equivalently the correlation matrix can be seen as the covariance matrix of the standardized random variables for each element on the principal diagonal of a correlation matrix is the correlation of a random variable with itself which always equals 1 each offdiagonal element is between 1 and –1 inclusive conflicting nomenclatures and notations  editnomenclatures differ some statisticians following the probabilist william feller in his twovolume book an introduction to probability theory and its applications 2 call the matrix the variance of the random vector because it is the natural generalization to higher dimensions of the 1dimensional variance others call it the covariance matrix because it is the matrix of covariances between the scalar components of the vector both forms are quite standard and there is no ambiguity between them the matrix is also often called the variancecovariance matrix since the diagonal terms are in fact variances by comparison the notation for the crosscovariance between two vectors is properties  editbasic properties  editfor and where x is a p dimensional random variable and y a q dimensional random variable the following basic properties apply 3is positivesemidefinite and symmetric if p  q then if and are independent or somewhat less restrictedly if every random variable in is uncorrelated with every random variable in  thenwhere and are p ×1 random vectors is a q ×1 random vector is a q ×1 vector is a p ×1 vector and and are q × p matrices of constants block matrices  editthe joint mean and joint covariance matrix of and can be written in block form  where andand can be identified as the variance matrices of the marginal distributions for and respectively if and are jointly normally distributedthen the conditional distribution for given is given by 4defined by conditional meanand conditional variance the matrix σ yx σ xx −1 is known as the matrix of regression coefficients while in linear algebra σ yx is the schur complement of σ xx in σ xythe matrix of regression coefficients may often be given in transpose form σ xx −1 σ xy suitable for postmultiplying a row vector of explanatory variables x t rather than premultiplying a column vector x in this form they correspond to the coefficients obtained by inverting the matrix of the normal equations of ordinary least squares ols covariance matrix as a parameter of a distribution  editif a vector of n possibly correlated random variables is jointly normally distributed or more generally elliptically distributed then its probability density function can be expressed in terms of the covariance matrix 5covariance matrix as a linear operator  editmain article covariance operator applied to one vector the covariance matrix maps a linear combination c of the random variables x onto a vector of covariances with those variables treated as a bilinear form it yields the covariance between the two linear combinations the variance of a linear combination is then its covariance with itself similarly the pseudoinverse covariance matrix provides an inner product which induces the mahalanobis distance a measure of the unlikelihood of c  citation neededwhich matrices are covariance matrices  editfrom the identity just above let be a realvalued vector thenwhich must always be nonnegative since it is the variance of a realvalued random variable from the symmetry of the covariance matrixs definition it follows that only a positivesemidefinite matrix can be a covariance matrix conversely every symmetric positive semidefinite matrix is a covariance matrix to see this suppose m is a p × p positivesemidefinite matrix from the finitedimensional case of the spectral theorem it follows that m has a nonnegative symmetric square root that can be denoted by m 12 let be any p ×1 column vectorvalued random variable whose covariance matrix is the p × p identity matrix then  citation neededcomplex random vectors  editthe variance of a complex scalarvalued random variable with expected value μ is conventionally defined using complex conjugationwhere the complex conjugate of a complex number is denoted thus the variance of a complex number is a real number if is a columnvector of complexvalued random variables then the conjugate transpose is formed by both transposing and conjugating in the following expression the product of a vector with its conjugate transpose results in a square matrix as its expectationwhere denotes the conjugate transpose which is applicable to the scalar case since the transpose of a scalar is still a scalar the matrix so obtained will be hermitian positivesemidefinite 6 with real numbers in the main diagonal and complex numbers offdiagonal estimation  editmain article estimation of covariance matrices if and are centred data matrices of dimension n by p and nbyq respectively ie with n rows of observations of p and q columns of variables from which the column means have been subtracted then if the column means were estimated from the data sample correlation matrices and can be defined to beor if the column means were known apriorithese empirical sample correlation matrices are the most straightforward and most often used estimators for the correlation matrices but other estimators also exist including regularised or shrinkage estimators which may have better properties applications  editthe covariance matrix is a useful tool in many different areas from it a transformation matrix can be derived called a whitening transformation that allows one to completely decorrelate the data  citation needed or from a different point of view to find an optimal basis for representing the data in a compact way  citation needed see rayleigh quotient for a formal proof and additional properties of covariance matrices this is called principal component analysis pca and the karhunenloève transform kltransform the covariance matrix plays a key role in financial economics especially in portfolio theory and its mutual fund separation theorem and in the capital asset pricing model the matrix of covariances among various assets returns is used to determine under certain assumptions the relative amounts of different assets that investors should in a normative analysis or are predicted to in a positive analysis choose to hold in a context of diversification see also  editcovariance mapping multivariate statistics gramian matrix eigenvalue decomposition quadratic form statisticsprincipal components references  edit wasserman larry 2004 all of statistics a concise course in statistical inference isbn 0387402721 william feller 1971 an introduction to probability theory and its applications wiley isbn 9780471257097 retrieved 10 august 2012 taboga marco 2010 lectures on probability theory and mathematical statistics eaton morris l 1983 multivariate statistics a vector space approach john wiley and sons pp 116–117 isbn 0471027766 frahm g junker m szimayer a 2003 elliptical copulas applicability and limitations statistics  probability letters 63 3 275–286 doi 101016s01677152 03000920 brookes mike the matrix reference manual further reading  edithazewinkel michiel ed 2001 1994 covariance matrix encyclopedia of mathematics springer sciencebusiness media b v  kluwer academic publishers isbn 9781556080104weisstein eric w covariance matrix math worldvan kampen n g 1981 stochastic processes in physics and chemistry new york northholland isbn 0444862005  hidev t e statistics outline index  showdescriptive statistics  showdata collection  showstatistical inference  showcorrelation regression analysis  showcategorical  multivariate  timeseries  survival analysis  showapplications category portal commons wiki project  showv t e matrix classes categories covariance and correlation matrices summary statistics 
introduction in this tutorial we will use the ambari hdfs file view to store data files of truck drivers statistics we will implement hive queries to analyze process and filter that data prerequisites downloaded and installed latest hortonworks sandbox learning the ropes of the hdp sandbox allow yourself around one hour to complete this tutorial outline hive hive or pig our data processing task step 1 download the data step 2 upload the data files step 3 start the hive view summary further reading hive apache hive is a component of hortonworks data platform hdp hive provides a sqllike interface to data stored in hdp in the previous tutorial we used pig which is a scripting language with a focus on dataflows hive provides a database query interface to apache hadoop hive or pig people often ask why do pig and hive exist when they seem to do much of the same thing hive because of its sql like query language is often used as the interface to an apache hadoop based data warehouse hive is considered friendlier and more familiar to users who are used to using sql for querying data pig fits in through its data flow strengths where it takes on the tasks of bringing data into apache hadoop and working with it to get it into the form for querying a good overview of how this works is in alan gates posting on the yahoo developer blog titled pig and hive at yahoo from a technical point of view both pig and hive are feature complete so you can do tasks in either tool however you will find one tool or the other will be preferred by the different groups that have to use apache hadoop the good part is they have a choice and both tools work together our data processing task we are going to do the same data processing task as we just did with pig in the previous tutorial we have several files of truck driver statistics and we are going to bring them into hive and do some simple computing with them we are going to compute the sum of hours and miles logged driven by a truck driver for an year once we have the sum of hours and miles logged we will extend the script to translate a driver id field into the name of the drivers by joining two different tables step 1 download the data download the driver data file from here once you have the file you will need to unzip the file into a directory we will be uploading two csv files – drivers csv and timesheet csv step 2 upload the data files we start by selecting the hdfs files view from the offcanvas menu at the top the hdfs files view allows us to view the hortonworks data platform hdp file store this is separate from the local file system for the hortonworks sandbox it will be part of the file system in the hortonworks sandbox vm navigate to user mariadev and click on the upload button to select the files we want to upload into the hortonworks sandbox environment click on the browse button to open a dialog box navigate to where you stored the drivers csv file on your local disk and select drivers csv and click open do the same thing for timesheet csv when you are done you will see there are two new files in your directory step 3 start the hive view let’s open the hive view 20 by clicking on the views icon on the top bar hive view 20 provides a user interface to the hive data warehouse system for hadoop31 explore the hive user interface below is the query editor a query may span multiple lines at the bottom there are buttons to execute the query visual explain the query save as the query with a name and to open a new worksheet window for another query hive and pig data model differences before we get started let’s take a look at how pig and hive data models differ in the case of pig all data objects exist and are operated on in the script once the script is complete all data objects are deleted unless you stored them in the case of hive we are operating on the apache hadoop data store any query you make table that you create data that you copy persists from query to query you can think of hive as providing a data workbench where you can examine modify and manipulate the data in apache hadoop so when we perform our data processing task we will execute it one query or line at a time once a line successfully executes you can look at the data objects to verify if the last operation did what you expected all your data is live compared to pig where data objects only exist inside the script unless they are copied out to storage this kind of flexibility is hive’s strength you can solve problems bit by bit and change your mind on what to do next depending on what you find32 create table tempdrivers the first task we will do is create a table to hold the data we will type the query into the query editor once you have typed in the query hit the execute button at the bottomcreate table tempdrivers  colvalue string hint press ctrl  space for autocompletion the query does not return any results because at this point we just created an empty table and we have not copied any data in it once the query has executed we can refresh the database by reselecting the database we will see the new table called tempdrivers33 create query to populate hive table tempdrivers with driverscsv data the next line of code will load the data file drivers csv into the table tempdrivers load data inpath usermariadevdriverscsv overwrite into table tempdriversafter executing load data we can see table tempdrivers was populated with data from drivers csv note that hive consumed the data file drivers csv during this step if you look in the file browser you will see driverscsv is no longer there34 create table drivers now that we have read the data in we can start working with it the next thing we want to do extract the data so first we will type in a query to create a new table called drivers to hold the data that table will have six columns for driver idname ssn location certified and the wage  plan of drivers create table drivers  driver id int name string ssn bigint location string certified string wageplan string 35 create query to extract data from tempdrivers and store it to drivers then we extract the data we want from tempdrivers and copy it into drivers we will do this with a regexp pattern to do this we are going to build up a multiline query the six regexpextract calls are going to extract the driver id namessn location certified and the wage  plan fields from the table tempdrivers when you are done typing the query it will look like this be careful as there are no spaces in the regular expression patterninsert overwrite table drivers select regexpextract  colvalue      1 1 driver idregexpextract  colvalue      2 1 nameregexpextract  colvalue      3 1 ssnregexpextract  colvalue      4 1 locationregexpextract  colvalue      5 1 certifiedregexpextract  colvalue      6 1 wageplanfrom tempdriversexecute the query and look at the drivers table you should see data that looks like this36 create temptimesheet and timesheet tables similarly similarly we have to create a table called temptimesheet then load the sample timesheet csv file type the following queries one by one create table temptimesheet  colvalue string load data inpath usermariadevtimesheetcsv overwrite into table temptimesheetyou should see the data like this now create the table timesheet using the following query create table timesheet  driver id int week int hourslogged int mileslogged int insert the data into the table timesheet from temptimesheet table using the same regexpextract as we did earlierinsert overwrite table timesheet select regexpextract  colvalue      1 1 driver idregexpextract  colvalue      2 1 weekregexpextract  colvalue      3 1 hoursloggedregexpextract  colvalue      4 1 milesloggedfrom temptimesheetyou should see the data like this37 create query to filter the data driver id hourslogged milesloggednow we have the data fields we want the next step is to group the data by driver id so we can find the sum of hours and miles logged score for an year this query first groups all the records by driver id and then selects the driver with the sum of the hours and miles logged runs for that year select driver id sum  hourslogged  sum  mileslogged from timesheet group by driver idthe results of the query look like this38 create query to join the data driver id name hourslogged milesloggednow we need to go back and get the driver id  s so we know who the driver s was we can take the previous query and join it with the drivers records to get the final table which will have the driver id name and the sum of hours and miles logged select d driver id d name t totalhours t totalmiles from drivers d join  select driver id sum  hourslogged totalhours sum  mileslogged totalmiles from timesheet group by driver id t on  d driver id  t driver id the resulting data looks like so now we have our results as described earlier we solved this problem using hive step by step at any time we were free to look around at the data decide we needed to do another task and come back at all times the data is live and accessible to us summary congratulations on completing this tutorial we just learned how to upload data into hdfs files view and create hive queries to manipulate data let’s review all the queries that were utilized in this tutorial create load insert select from group by join and on with these queries we created a table tempdrivers to store the data we created another table drivers so we can overwrite that table with extracted data from the tempdrivers table we created earlier then we did the same for temptimesheet and timesheet  finally created queries to filter the data to have the result show the sum of hours and miles logged by each driver further reading apache hive hive tutorials hive language manual
keras as a simplified interface to tensor flow tutorial sun 24 april 2016by francois chollet in tutorials a complete guide to using keras as part of a tensor flow workflow if tensor flow is your primary framework and you are looking for a simple  highlevel model definition interface to make your life easier this tutorial is for you keras layers and models are fully compatible with puretensor flow tensors and as a result keras makes a great model definition addon for tensor flow and can even be used alongside other tensor flow libraries lets see how note that this tutorial assumes that you have configured keras to use the tensor flow backend instead of theano here are instructions on how to do this we will cover the following points i calling keras layers on tensor flow tensors ii using keras models with tensor flow iii multigpu and distributed training iv exporting a model with tensor flowserving i calling keras layers on tensor flow tensors lets start with a simple example mnist digits classification we will build a tensor flow digits classifier using a stack of keras dense layers fullyconnected layers we should start by creating a tensor flow session and registering it with keras this means that keras will use the session we registered to initialize all variables that it creates internallyimport tensorflow as tfsess  tf session from keras import backend as kk setsession  sessnow lets get started with our mnist model we can start building a classifier exactly as you would do in tensor flow this placeholder will contain our input digits as flat vectorsimg  tf placeholder  tf float32 shape   none 784 we can then use keras layers to speed up the model definition processfrom keraslayers import dense keras layers can be called on tensor flow tensorsx  dense  128 activation  relu   img  fullyconnected layer with 128 units and re lu activationx  dense  128 activation  relu   xpreds  dense  10 activation  softmax   x  output layer with 10 units and a softmax activation we define the placeholder for the labels and the loss function we will uselabels  tf placeholder  tf float32 shape   none 10 from kerasobjectives import categoricalcrossentropyloss  tf reducemean  categoricalcrossentropy  labels preds lets train the model with a tensor flow optimizerfrom tensorflowexamplestutorialsmnist import inputdatamnistdata  inputdata readdatasets  mnistdata onehot  truetrainstep  tf train gradient descent optimizer  05 minimize  loss initialize all variablesinitop  tf globalvariablesinitializer sess run  initop run training loopwith sess asdefault for i in range  100 batch  mnistdata train nextbatch  50trainstep run  feeddict   img batch  0 labels batch  1 we can now evaluate the modelfrom kerasmetrics import categoricalaccuracy as accuracyaccvalue  accuracy  labels predswith sess asdefault print accvalue eval  feeddict   img mnistdata test imageslabels mnistdata test labels in this case we use keras only as a syntactical shortcut to generate an op that maps some tensor s input to some tensor s output and thats it the optimization is done via a native tensor flow optimizer rather than a keras optimizer we dont even use any keras model at alla note on the relative performance of native tensor flow optimizers and keras optimizers there are slight speed differences when optimizing a model the keras way vs with a tensor flow optimizer somewhat counterintuitively keras seems faster most of the time by 510 however these differences are small enough that it doesnt really matter at the end of the day whether you optimize your models via keras optimizers or native tf optimizers different behaviors during training and testing some keras layers eg dropout batch normalization behave differently at training time and testing time you can tell whether a layer uses the learning phase traintest by printing layeruseslearningphase a boolean true if the layer has a different behavior in training mode and test mode false otherwise if your model includes such layers then you need to specify the value of the learning phase as part of feeddict so that your model knows whether to apply dropoutetc or not the keras learning phase a scalar tensor flow tensor is accessible via the keras backendfrom keras import backend as kprint k learningphase to make use of the learning phase simply pass the value 1 training mode or 0 test mode to feeddict train modetrainstep run  feeddict   x batch  0  labels batch  1  k learningphase  1 for instance heres how to add dropout layers to our previous mnist examplefrom keraslayers import dropoutfrom keras import backend as kimg  tf placeholder  tf float32 shape   none 784 labels  tf placeholder  tf float32 shape   none 10 x  dense  128 activation  relu   imgx  dropout  05   xx  dense  128 activation  relu   xx  dropout  05   xpreds  dense  10 activation  softmax   xloss  tf reducemean  categoricalcrossentropy  labels preds trainstep  tf train gradient descent optimizer  05 minimize  losswith sess asdefault for i in range  100 batch  mnistdata train nextbatch  50trainstep run  feeddict   img batch  0 labels batch  1 k learningphase  1 accvalue  accuracy  labels predswith sess asdefault print accvalue eval  feeddict   img mnistdata test imageslabels mnistdata test labelsk learningphase  0 compatibility with name scopes device scopes keras layers and models are fully compatible with tensor flow name scopes for instance consider the following code snippetx  tf placeholder  tf float32 shape   none 20 64 with tf namescope  block1 y  lstm  32 name  mylstm   xthe weights of our lstm layer will then be named block1mylstmwi block1mylstmui etc similarly device scopes work as you would expectwith tf device  gpu0 x  tf placeholder  tf float32 shape   none 20 64 y  lstm  32   x  all ops  variables in the lstm layer will live on gpu0compatibility with graph scopes any keras layer or model that you define inside a tensor flow graph scope will have all of its variables and operations created as part of the specified graph for instance the following works as you would expectfrom keraslayers import lstmimport tensorflow as tfmygraph  tf graph with mygraph asdefault x  tf placeholder  tf float32 shape   none 20 64 y  lstm  32   x  all ops  variables in the lstm layer are created as part of our graph compatibility with variable scopes variable sharing should be done via calling a same keras layer or model instance multiple times not via tensor flow variable scopes a tensor flow variable scope will have no effect on a keras layer or model for more information about weight sharing with keras please see the weight sharing section in the functional api guide to summarize quickly how weight sharing works in keras by reusing the same layer instance or model instance you are sharing its weights heres a simple example instantiate a keras layerlstm  lstm  32 instantiate two tf placeholdersx  tf placeholder  tf float32 shape   none 20 64 y  tf placeholder  tf float32 shape   none 20 64  encode the two tensors with the same lstm weightsxencoded  lstm  xyencoded  lstm  ycollecting trainable weights and state updates some keras layers stateful rnns and batch normalization layers have internal updates that need to be run as part of each training step there are stored as a list of tensor tuples layerupdates you should generate assign ops for those to be run at each training step heres an examplefrom keraslayers import batch normalizationlayer  batch normalization   xupdateops  for oldvalue newvalue in layer updatesupdateops append  tf assign  oldvalue newvalue note that if you are using a keras model  model instance or sequential instance modeludpates behaves in the same way and collects the updates of all underlying layers in the model in addition in case you need to explicitly collect a layers trainable weights you can do so via layertrainableweights or modeltrainableweights  a list of tensor flow variable instancesfrom keraslayers import denselayer  dense  32   x  instantiate and call a layerprint layer trainableweights  list of tensor flow variables knowing this allows you to implement your own training routine based on a tensor flow optimizer ii using keras models with tensor flow converting a keras sequential model for use in a tensor flow workflow you have found a keras sequential model that you want to reuse in your tensor flow project consider for instance this vgg16 image classifier with pretrained weights  how to proceed first of all note that if your pretrained weights include convolutions layers convolution2d or convolution1d that were trained with theano you need to flip the convolution kernels when loading the weights this is due theano and tensor flow implementing convolution in different ways tensor flow actually implements correlation much like caffe heres a short guide on what you need to do in this case lets say that you are starting from the following keras model and that you want to modify so that it takes as input a specific tensor flow tensor myinputtensor this input tensor could be a data feeder op for instance or the output of a previous tensor flow model this is our initial keras modelmodel  sequential model add  dense  32 activation  relu inputdim  784 model add  dense  10 activation  softmax you just need to use keraslayers input layer to start building your sequential model on top of a custom tensor flow placeholder then build the rest of the model on topfrom keraslayers import input layer this is our modified keras modelmodel  sequential model add  input layer  inputtensor  custominputtensorinputshape   none 784  build the rest of the model as beforemodel add  dense  32 activation  relu model add  dense  10 activation  softmax at this stage you can call modelloadweights weightsfile to load your pretrained weights then you will probably want to collect the sequential models output tensoroutputtensor  model output you can now add new tensor flow ops on top of outputtensor etc calling a keras model on a tensor flow tensor a keras model acts the same as a layer and thus can be called on tensor flow tensorsfrom kerasmodels import sequentialmodel  sequential model add  dense  32 activation  relu inputdim  784 model add  dense  10 activation  softmax  this worksx  tf placeholder  tf float32 shape   none 784 y  model  xnote by calling a keras model your are reusing both its architecture and its weights when you are calling a model on a tensor you are creating new tf ops on top of the input tensor and these ops are reusing the tf variable instances already present in the model iii multigpu and distributed training assigning part of a keras model to different gpus tensor flow device scopes are fully compatible with keras layers and models hence you can use them to assign specific parts of a graph to different gpus heres a simple examplewith tf device  gpu0 x  tf placeholder  tf float32 shape   none 20 64 y  lstm  32   x  all ops in the lstm layer will live on gpu0with tf device  gpu1 x  tf placeholder  tf float32 shape   none 20 64 y  lstm  32   x  all ops in the lstm layer will live on gpu1note that the variables created by the lstm layers will not live on gpu all tensor flow variables always live on cpu independently from the device scope where they were created tensor flow handles devicetodevice variable transfer behind the scenes if you want to train multiple replicas of a same model on different gpus while sharing the same weights across the different replicas you should first instantiate your model or layers under one device scope then call the same model instance multiple times in different gpu device scopes such aswith tf device  cpu0 x  tf placeholder  tf float32 shape   none 784  shared model living on cpu0 it wont actually be run during training it acts as an op template and as a repository for shared variablesmodel  sequential model add  dense  32 activation  relu inputdim  784 model add  dense  10 activation  softmax  replica 0with tf device  gpu0 output0  model  x  all ops in the replica will live on gpu0 replica 1with tf device  gpu1 output1  model  x  all ops in the replica will live on gpu1 merge outputs on cpuwith tf device  cpu0 preds  05   output0  output1 we only run the preds tensor so that only the two replicas on gpu get run plus the merge op on cpuoutputvalue  sess run   preds  feeddict   x data distributed training you can trivially make use of tensor flow distributed training by registering with keras a tf session linked to a clusterserver  tf train server createlocalserver sess  tf session  server targetfrom keras import backend as kk setsession  sessfor more information about using tensor flow in a distributed setting see this tutorial iv exporting a model with tensor flowserving tensor flow serving is a library for serving tensor flow models in a production setting developed by google any keras model can be exported with tensor flowserving as long as it only has one input and one output which is a limitation of tfserving whether or not it was training as part of a tensor flow workflow in fact you could even train your keras model with theano then switch to the tensor flow keras backend and export your model heres how it works if your graph makes use of the keras learning phase different behavior at training time and test time the very first thing to do before exporting your model is to hardcode the value of the learning phase as 0 presumably ie test mode into your graph this is done by 1 registering a constant learning phase with the keras backend and 2 rebuilding your model afterwards here are these two simple steps in actionfrom keras import backend as kk setlearningphase  0  all new operations will be in test mode from now on serialize the model and get its weights for quick rebuildingconfig  previousmodel getconfig weights  previousmodel getweights  rebuild a model where the learning phase is now hardcoded to 0from kerasmodels import modelfromconfignewmodel  modelfromconfig  confignewmodel setweights  weightswe can now use tensor flowserving to export the model following the instructions found in the official tutorialfrom tensorflowservingsessionbundle import exporterexportpath    where to save the exported graphexportversion    version number integersaver  tf train saver  sharded  truemodelexporter  exporter exporter  saversignature  exporter classificationsignature  inputtensor  model inputscorestensor  model outputmodelexporter init  sess graph asgraphdef defaultgraphsignature  signaturemodelexporter export  exportpath tf constant  exportversion  sesswant to see a new topic covered in this guide reach out on twitter 
docs previous versions windows understanding mpio features and components june 27 2012 19 minutes to read in this article about mpiomultipath solutions in windows server high availability solutions application availability through failover clustering high availability through mpioconsiderations for using mpiomaking mpiobased solutions work device discovery and enumeration unique storage device identifier dynamic load balancing error handling failover and recovery differences in loadbalancing technologies differences in failover technologies scenario 1 using mpio without failover clustering scenario 2 combining the use of mpio in fault tolerant mode with failover clustering scenario 3 combining the use of mpio in loadbalancing mode with failover clustering about the windows storage stack and drivers storage stack and device drivers device drivers port drivers miniport drivers class drivers multipath bus drivers mpiosysdsm management mpio dsmdevice initialization mpio device discovery request handling error handling details about the microsoft dsm in windows server applies to windows server 2008 r2 windows server 2012about mpiomicrosoft multipath io mpio is a microsoftprovided framework that allows storage providers to develop multipath solutions that contain the hardwarespecific information needed to optimize connectivity with their storage arrays these modules are called devicespecific modules dsms the concepts around dsms are discussed later in this document mpio is protocolindependent and can be used with fibre channel internet scsi i scsi and serial attached scsi sas interfaces in windows server® 2008 windows server 2008 r2 and windows server 2012 multipath solutions in windows server when running on windows server 2008 r2 an mpio solution can be deployed in the following ways by using a dsm provided by a storage array manufacturer for windows server in a fibre channel i scsi or sas shared storage configuration by using the microsoft dsm which is a generic dsm provided for windows server in a fibre channel i scsi or sas shared storage configuration note to work with the microsoft dsm storage must be scsi primary commands3 spc3 compliant high availability solutions keeping missioncritical data continuously available has become a requirement over a wide range of customer segments from small business to datacenter environments enterprise environments that use windows server require no downtime for key workloads including file server database messaging and other line of business applications this level of availability can be difficult and very costly to achieve and it requires that redundancy be built in at multiple levels storage redundancy backups to separate recovery servers server clustering and redundancy of the physical path components application availability through failover clustering clustering is the use of multiple servers host bus adapters hbas and storage devices that work together to provide users with high application availability if a server experiences a hardware failure or is temporarily unavailable end users are still able to transparently access data or applications on a redundant cluster node in addition to providing redundancy at the server level clustering can also be used as a tool to minimize the downtime required for patch management and hardware maintenance clustering solutions require software that enables transparent failover between systems failover clustering formerly known as microsoft cluster server mscs is one such solution that is included with the windows server 2008 r2 and windows server 2012 operating systems high availability through mpiompio allows windows® to manage and efficiently use up to 32 paths between storage devices and the windows host operating system although both mpio and failover clustering result in high availability and improved performance they are not equivalent concepts while failover clustering provides high application availability and tolerance of server failure mpio provides fault tolerant connectivity to storage by employing mpio and failover clustering together as complimentary technologies users are able to mitigate the risk of a system outage at both the hardware and application levels mpio provides the logical facility for routing io over redundant hardware paths connecting server to storage these redundant hardware paths are made up of components such as cabling host bus adapters hbas switches storage controllers and possibly even power mpio solutions logically manage these redundant connections so that io requests can be rerouted if a component along one path fails as more and more data is consolidated on storage area networks sans the potential loss of access to storage resources is unacceptable to mitigate this risk high availability solutions such as mpio have now become a requirement considerations for using mpioconsider the following when using mpio when using the microsoft dsm storage that implements an activeactive storage scheme but does not support alua will default to use the round robin loadbalancing policy setting although a different policy setting may be chosen later additionally you can preconfigure mpio so that when it detects a certain hardware id it defaults to a specific loadbalancing policy setting for more information about loadbalancing policy settings see referencing mpclaim examples windows multipathing solutions are required if you want to utilize the mpio framework to be eligible to receive logo qualification for windows server for additional information about windows logo requirements see windows quality online services winqual  httpgomicrosoftcomfwlink link id71551  this joint solution allows storage partners to design hardware solutions that are integrated with the windows operating system compatibility with both the operating system and other partner provided storage devices is ensured through the windows logo program tests to help ensure proper storage device functionality this ensures a highly available multipath solution by using mpio which offers supportability across windows operating system implementations to determine which dsm to use with your storage refer to information from your hardware storage array manufacturer multipath solutions are supported as long as a dsm is implemented in line with logo requirements for mpio most multipath solutions for windows today use the mpio architecture and a dsm provided by the storage array manufacturer you can use the microsoft dsm provided by microsoft in windows server if it is also supported by the storage array manufacturer refer to your storage array manufacturer for information about which dsm to use with a given storage array as well as the optimal configuration of it multipath software suites available from storage array manufacturers may provide an additional valueadd beyond the implementation of the microsoft dsm because the software typically provides autoconfiguration heuristics for specific storage arrays statistical analysis and integrated management we recommend using the dsm provided by the hardware storage array manufacturer to achieve optimal performance because the storage array manufacturer can make more advanced path decisions in their dsm that are specific to their array which may result in quicker path failover times making mpiobased solutions work the windows operating system relies on the plug and play pn p manager to dynamically detect and configure hardware such as adapters or disks including hardware used for high availabilityhigh performance multipath solutions note you might be prompted to restart the computer after the mpio feature is first installed device discovery and enumeration an mpiomultipath driver cannot work effectively or efficiently until it discovers enumerates and configures different devices that the operating system sees through redundant adapters into a logical group we will briefly outline in this section how mpio works with dsm in discovering and configuring the devices without any multipath driver the same devices through different physical paths would appear as totally different devices thereby leaving room for data corruption figure 1 depicts this scenario figure 1 multipathing software and storage unit distinction following is the sequence of steps that the device driver stack walks through in discovering enumerating and grouping the physical devices and device paths into a logical set this assumes a scenario where a new device is presented to the server a new device arrives the pn p manager detects the device’s arrival the mpio driver stack is notified of the device’s arrival it takes further action if it is a supported mpio device the mpio driver stack creates a pseudo device for the physical device the mpio driver walks through all the available dsms to determine which vendorspecific dsm can claim the device after a dsm claims a device it is associated only with the dsm that claimed it the mpio driver along with the dsm verifies that the path to the device is connected active and ready for io if a new path for this same device arrives mpio then works with the dsm to determine whether this device is the same as any other claimed device it then groups this physical path for the same device into a logical set for the multipath group that is called a pseudological unit number pseudolun unique storage device identifier for dynamic discovery to work correctly some form of identifier must be identified and obtainable regardless of the path from the host to the storage device each logical unit must have a unique hardware identifier the mpio driver package does not use disk signatures placed in the data area of a disk for identification purposes by software instead the microsoftprovided generic dsm generates a unique identifier from the data that is provided by the storage hardware mpio also provides for optionally using a unique hardware identifier assigned by the device manufacturer dynamic load balancing load balancing the redistribution of readwrite requests for the purpose of maximizing throughput between server and storage device is especially important in high workload settings or other settings where consistent service levels are critical without mpio software a server sending io requests down several paths may operate with very heavy workloads on some paths while others are underutilized the mpio software supports the ability to balance io workload without administrator intervention mpio determines which paths to a device are in an active state and can be used for load balancing each vendor’s loadbalancing policy setting which may use any of several algorithms such as round robin the path with the fewest outstanding commands or a vendor unique algorithm is set in the dsm this policy setting determines how the io requests are actually routed note in addition to the support for load balancing provided by mpio the hardware used must support the ability to use multiple paths at the same time rather than just fault tolerance error handling failover and recovery the mpio driver in combination with the dsm supports endtoend path failover the process of detecting failed paths and recovering from the failure is automatic usually fast and completely transparent to the it organization the data ideally remains available at all times not all errors result in failover to a new path some errors are temporary and can be recovered by using a recovery routine in the dsm if recovery is successful mpio is notified and path validity is checked to verify that it can be used again to transmit io requests when a fatal error occurs the path is invalidated and a new path is selected the io is resubmitted on this new path without requiring the application layer to resubmit the data differences in loadbalancing technologies there are two primary types of loadbalancing technologies referred to within windows this document discusses only mpio load balancing mpio load balancing is a type of load balancing supported by mpio that uses multiple data paths between server and storage to provide greater throughput of data than could be achieved with only one connection network load balancing nlb is a failover cluster technology formerly known as mscs that provides load balancing of network interfaces to provide greater throughput across a network to the server and is most typically used with internet information services iis differences in failover technologies when addressing data path failover such as the failover of host bus adapter hba or i scsi connections to storage the following main types of failover are available mpiobased fault tolerant failover in this scenario multiple data paths to the storage are configured and in the event that one path fails hba or the network adapter is able to fail over to the other path and resend any outstanding io for a server that has one or more hbas or network adapters mpio provides the following support for redundant switch fabrics or connections from the switch to the storage array protection against the failure of one of the adapters within the server directly mpiobased load balancing in this scenario multiple paths to storage are also defined however the dsm is able to balance the data load to maximize throughput this configuration can also employ fault tolerant behavior so that if one path fails all data would follow an alternate path in some hardware configurations you may have the ability to perform dynamic firmware updates on the storage controller such that a complete outage is not required for firmware updates this capability is hardware dependent and requires at a minimum that more than one storage controller be present on the storage so that data paths can be moved off of a storage controller for upgrades failover clustering this type of configuration offers resource failover at the application level from one cluster server node to another this type of failover is more invasive than storage path failover because it requires client applications to reconnect after failover and then resend data from the application layer this method can be combined with mpiobased fault tolerant failover and mpiobased load balancing to further mitigate the risk of exposure to different types of hardware failures different behaviors are available depending on the type of failover technology used and whether it is combined with a different type of failover or redundancy consider the following scenarios scenario 1 using mpio without failover clustering this scenario provides for either a fault tolerant connection to data or a loadbalanced connection to storage since this layer of fault tolerant operation protects only the connectivity between the server and storage it does not provide protection against server failure scenario 2 combining the use of mpio in fault tolerant mode with failover clustering this configuration provides the following advantages if a path to the storage fails mpio can use an alternate path without requiring client application reconnection if an individual server experiences a critical event such as hardware failure the application managed by failover clustering is failed over to another cluster node while this scenario requires client reconnection the time to restore the service may be much shorter than that required for replacing the failed hardware scenario 3 combining the use of mpio in loadbalancing mode with failover clustering this scenario provides the same benefits as listed in scenario 2 plus the following benefit during normal operation multiple data paths may be employed to provide greater aggregate throughput than one path can provide about the windows storage stack and drivers for the operating system to correctly perform operations that relate to hardware such as addition or removal of devices or transferring io requests from an application to a storage device the correct device drivers must be associated with the device all devicerelated functionality is initiated by the operating system but under direct control of subroutines contained within each driver these processes are considerably complicated when there are multiple paths to a device the mpio software prevents data corruption by ensuring correct handling of the driver associated with a single device that is visible to the operating system through multiple paths data corruption is likely to occur because when an operating system believes two separate paths lead to two separate storage volumes it does not enforce any serialization or prevent any cache conflicts consider what would happen if a new ntfs file system tries to initialize its journal log twice on a single volume storage stack and device drivers storage architecture in windows consists of a series of layered drivers as shown in figure 2 note that the application and the disk subsystem are not part of the storage layers when a device such as a storage disk is first added in each layer of the hierarchy is responsible for making the disk functional such as by adding partitions volumes and the file system the stack layers below the broken line are collectively known as the device stack and deal directly with managing storage devices figure 2 layered drivers in windows storage architecture device drivers device drivers manage specific hardware devices such as a disks or tapes on behalf of the operating system port drivers port drivers manage different types of transport depending on the type of adapter for example usb i scsi or fibre channel in use historically one of the most common port drivers in the windows system was the scsiport driver in conjunction with the class driver the port driver handles plug and play pn p and power functionality port drivers manage the connection between the device and the bus windows server 2003 introduced a new port driver storport which is better suited to highperformance highreliability environments and is typically more commonly used today than scsiport miniport drivers each storage adapter has an associated device driver known as a miniport this driver implements only those routines necessary to interface with the storage adapter’s hardware a miniport partners with a port driver to implement a complete layer in the storage stack as shown in figure 2 class drivers class drivers manage a specific device type they are responsible for presenting a unified disk interface to the layers above for example to control readwrite behavior for a disk the class driver manages the functionality of the device class drivers like port and miniport drivers are not a part of the mpio driver package per se however the pn p disk class driver disksys is used as part of the multipathing solution because the class driver controls the disk addremoval process and io requests pass through this driver to the mpio bus driver for more information see the mpio drivers sections that follow the mpio driver is implemented in the kernel mode of the operating system it works in combination with the pn p manager the disk class driver the port driver the miniport driver and a devicespecific module dsm to provide full multipath functionality multipath bus drivers mpiosysbus drivers are responsible for managing the connection between the device and the host computer the multipath bus driver provides a “software bus also technically termed a “root bus””—the conceptual analog to an actual bus slot into which a device plugs it acts as the parent bus for the multipath children disk pdos as a root bus mpiosys can create new device objects that are not created by new hardware being added into the configuration the mpio bus driver also communicates with the rest of the operating system and manages the pn p connection and power control between the hardware devices and the host computer and uses wmi classes to allow storage array manufacturers to monitor and manage their storage and associated dsms for more information about wmi see mpio wmi classes  httpgomicrosoftcomfwlink link id163826  dsm management management and monitoring of the dsm can be done through the windows management instrumentation wmi interface or by using the mpclaimexe tool the latter of which includes the required wmi code within the mpio drivers mpio dsmas explained previously in this document a storage array manufacturer’s devicespecific module dsm incorporates knowledge of the manufacturer’s hardware a dsm interacts with the mpio driver the dsm plays a crucial role in device initialization and io request handling including io request error handling these dsm actions are described further in the following sections device initialization mpio allows for devices from different storage vendors to coexist and be connected to the same windows serverbased system this means that a single server running windows server can have multiple dsms installed on it when a new eligible device is detected via pn p mpio attempts to determine which dsm is appropriate to handle the device mpio contacts each dsm one at a time the first dsm to claim ownership of the device is associated with that device and the remaining dsms are not allowed a chance to press claims for that already claimed device there is no particular order in which the dsms are contacted although the microsoft dsm is always contacted last if the dsm does support the device it then indicates whether the device is a new installation or is the same device previously installed but is now visible through a new path mpio device discovery figure 3 illustrates how devices and path discovery work with mpio figure 3 devices and path discovery with mpiorequest handling when an application makes an io request to a specific device the dsm that claimed the device makes a determination based on its internal loadbalancing algorithms as to which path the request should be sent error handling if the io request fails the dsm is responsible for analyzing the failure to determine whether to retry the io to cause a failover to a new path or to return the error to the requesting application in case of a failover the dsm determines what new path should be used the actual rebuild of the io and resubmission of the io is done by mpio and is not the responsibility of the dsm the details of the dsmmpio interaction to make all of this happen are beyond the scope of this document and are provided in the mpio driver development kit ddk available from microsoft details about the microsoft dsm in windows server the microsoft devicespecific module dsm provided in windows server includes support for the following policy settings failover only policy setting that does not perform load balancing this policy setting uses a single active path and the rest of the paths are standby paths the active path is used for sending all io if the active path fails one of the standby paths is used when the path that failed is reactivated or reconnected the standby path can optionally return to standby if failback is turned on for more information about how to configure mpio path automatic failback see the section later in this document titled “configure the mpio failback policy”round robin loadbalancing policy setting that allows the dsm to use all available paths for mpio in a balanced way this is the default policy that is chosen when the storage controller follows the activeactive model and the management application does not specifically choose a loadbalancing policy setting round robin with subset loadbalancing policy setting that allows the application to specify a set of paths to be used in a round robin fashion and with a set of standby paths the dsm uses paths from active paths for processing requests as long as at least one of the paths is available the dsm uses a standby path only when all of the active paths fail for example given 4 paths a b c and d paths a b and c are listed as active paths and d is the standby path the dsm chooses a path from a b and c in round robin fashion as long as at least one of them is available if all three paths fail the dsm uses d the standby path if paths a b or c become available the dsm stops using path d and switches to the available paths among a b and c least queue depth loadbalancing policy setting that sends io down the path with the fewest currently outstanding io requests for example consider that there is one io that is sent to lun 1 on path 1 and the other io is sent to lun 2 on path 1 the cumulative outstanding io on path 1 is 2 and on path 2 it is 0 therefore the next io for either lun will process on path 2 weighted paths loadbalancing policy setting that assigns a weight to each path the weight indicates the relative priority of a given path the larger the number the lower ranked the priority the dsm chooses the leastweighted path from among the available paths least blocks loadbalancing policy setting that sends io down the path with the least number of data blocks currently being processed for example consider that there are two ios one is 10 bytes and the other is 20 bytes both are in process on path 1 and there are no outstanding ios on path 2 the cumulative outstanding amount of io on path 1 is 30 bytes on path 2 it is 0 therefore the next io will process on path 2 share  theme
chapter 4 analyzing qualitative data what is qualitative analysis qualitative modes of data analysis provide ways of discerning examining comparing and contrasting and interpreting meaningful patterns or themes meaningfulness is determined by the particular goals and objectives of the project at hand the same data can be analyzed and synthesized from multiple angles depending on the particular research or evaluation questions being addressed the varieties of approaches  including ethnography narrative analysis discourse analysis and textual analysis  correspond to different types of data disciplinary traditions objectives and philosophical orientations however all share several common characteristics that distinguish them from quantitative analytic approaches in quantitative analysis numbers and what they stand for are the material of analysis by contrast qualitative analysis deals in words and is guided by fewer universal rules and standardized procedures than statistical analysis we have few agreedon canons for qualitative data analysis in the sense of shared ground rules for drawing conclusions and verifying their sturdiness miles and huberman 1984 this relative lack of standardization is at once a source of versatility and the focus of considerable misunderstanding that qualitative analysts will not specify uniform procedures to follow in all cases draws critical fire from researchers who question whether analysis can be truly rigorous in the absence of such universal criteria in fact these analysts may have helped to invite this criticism by failing to adequately articulate their standards for assessing qualitative analyses or even denying that such standards are possible their stance has fed a fundamentally mistaken but relatively common idea of qualitative analysis as unsystematic undisciplined and purely subjective although distinctly different from quantitative statistical analysis both in procedures and goals good qualitative analysis is both systematic and intensely disciplined if not objective in the strict positivist sense qualitative analysis is arguably replicable insofar as others can be walked through the analysts thought processes and assumptions timing also works quite differently in qualitative evaluation quantitative evaluation is more easily divided into discrete stages of instrument development data collection data processing and data analysis by contrast in qualitative evaluation data collection and data analysis are not temporally discrete stages as soon as the first pieces of data are collected the evaluator begins the process of making sense of the information moreover the different processes involved in qualitative analysis also overlap in time part of what distinguishes qualitative analysis is a looplike pattern of multiple rounds of revisiting the data as additional questions emerge new connections are unearthed and more complex formulations develop along with a deepening understanding of the material qualitative analysis is fundamentally an iterative set of processes at the simplest level qualitative analysis involves examining the assembled relevant data to determine how they answer the evaluation question s at hand however the data are apt to be in formats that are unusual for quantitative evaluators thereby complicating this task in quantitative analysis of survey results for example frequency distributions of responses to specific items on a questionnaire often structure the discussion and analysis of findings by contrast qualitative data most often occur in more embedded and less easily reducible or distillable forms than quantitative data for example a relevant piece of qualitative data might be interspersed with portions of an interview transcript multiple excerpts from a set of field notes or a comment or cluster of comments from a focus group throughout the course of qualitative analysis the analyst should be asking and reasking the following questions what patterns and common themes emerge in responses dealing with specific items how do these patterns or lack thereof help to illuminate the broader study question s are there any deviations from these patterns if yes are there any factors that might explain these atypical responses what interesting stories emerge from the responses how can these stories help to illuminate the broader study question s do any of these patterns or findings suggest that additional data may need to be collected do any of the study questions need to be revised do the patterns that emerge corroborate the findings of any corresponding qualitative analyses that have been conducted if not what might explain these discrepancies two basic forms of qualitative analysis essentially the same in their underlying logic will be discussed intracase analysis and crosscase analysis a case may be differently defined for different analytic purposes depending on the situation a case could be a single individual a focus group session or a program site berkowitz 1996 in terms of the hypothetical project described in chapter 2 a case will be a single campus intracase analysis will examine a single project site and crosscase analysis will systematically compare and contrast the eight campuses processes in qualitative analysis qualitative analysts are justifiably wary of creating an unduly reductionistic or mechanistic picture of an undeniably complex iterative set of processes nonetheless evaluators have identified a few basic commonalities in the process of making sense of qualitative data in this chapter we have adopted the framework developed by miles and huberman 1994 to describe the major phases of data analysis data reduction data display and conclusion drawing and verification data reduction first the mass of data has to be organized and somehow meaningfully reduced or reconfigured miles and huberman 1994 describe this first of their three elements of qualitative data analysis as data reduction data reduction refers to the process of selecting focusing simplifying abstracting and transforming the data that appear in written up field notes or transcriptions not only do the data need to be condensed for the sake of manageability they also have to be transformed so they can be made intelligible in terms of the issues being addressed data reduction often forces choices about which aspects of the assembled data should be emphasized minimized or set aside completely for the purposes of the project at hand beginners often fail to understand that even at this stage the data do not speak for themselves a common mistake many people make in quantitative as well as qualitative analysis in a vain effort to remain perfectly objective is to present a large volume of unassimilated and uncategorized data for the readers consumption in qualitative analysis the analyst decides which data are to be singled out for description according to principles of selectivity this usually involves some combination of deductive and inductive analysis while initial categorizations are shaped by preestablished study questions the qualitative analyst should remain open to inducing new meanings from the data available in evaluation such as the hypothetical evaluation project in this handbook data reduction should be guided primarily by the need to address the salient evaluation question s this selective winnowing is difficult both because qualitative data can be very rich and because the person who analyzes the data also often played a direct personal role in collecting them the words that make up qualitative analysis represent real people places and events far more concretely than the numbers in quantitative data sets a reality that can make cutting any of it quite painful but the acid test has to be the relevance of the particular data for answering particular questions for example a formative evaluation question for the hypothetical study might be whether the presentations were suitable for all participants focus group participants may have had a number of interesting things to say about the presentations but remarks that only tangentially relate to the issue of suitability may have to be bracketed or ignored similarly a participantâ€™s comments on his department chair that are unrelated to issues of program implementation or impact however fascinating should not be incorporated into the final report the approach to data reduction is the same for intracase and crosscase analysis with the hypothetical project of chapter 2 in mind it is illustrative to consider ways of reducing data collected to address the question what did participating faculty do to share knowledge with nonparticipating faculty the first step in an intracase analysis of the issue is to examine all the relevant data sources to extract a description of what they say about the sharing of knowledge between participating and nonparticipating faculty on the one campus included might be information from focus groups observations and indepth interviews of key informants such as the department chair the most salient portions of the data are likely to be concentrated in certain sections of the focus group transcripts or writeups and indepth interviews with the department chair however it is best to also quickly peruse all notes for relevant data that may be scattered throughout in initiating the process of data reduction the focus is on distilling what the different respondent groups suggested about the activities used to share knowledge between faculty who participated in the project and those who did not how does what the participating faculty say compare to what the nonparticipating faculty and the department chair report about knowledge sharing and adoption of new practices in setting out these differences and similarities it is important not to so flatten or reduce the data that they sound like closeended survey responses the tendency to treat qualitative data in this manner is not uncommon among analysts trained in quantitative approaches not surprisingly the result is to make qualitative analysis look like watered down survey research with a tiny sample size approaching qualitative analysis in this fashion unfairly and unnecessarily dilutes therichness of the data and thus inadvertently undermines one of the greatest strengths of the qualitative approach answering the question about knowledge sharing in a truly qualitative way should go beyond enumerating a list of knowledgesharing activities to also probe the respondents assessments of the relative effectiveness of these activities as well as their reasons for believing some more effective than others apart from exploring the specific content of therespondents views it is also a good idea to take note of the relative frequency with which different issues are raised as well as the intensity with which they are expressed data display data display is the second element or level in miles and hubermans 1994 model of qualitative data analysis data display goes a step beyond data reduction to provide an organized compressed assembly of information that permits conclusion drawing a display can be an extended piece of text or a diagram chart or matrix that provides a new way of arranging and thinking about the more textually embedded data data displays whether in word or diagrammatic form allow the analyst to extrapolate from the data enough to begin to discern systematic patterns and interrelationships at the display stage additional higher order categories or themes may emerge from the data that go beyond those first discovered during the initial process of data reduction from the perspective of program evaluation data display can be extremely helpful in identifying why a system eg a given program or project is or is not working well and what might be done to change it the overarching issue of why some projects work better or are more successful than others almost always drives the analytic process in any evaluation in our hypothetical evaluation example faculty from all eight campuses come together at the central campus to attend workshops in that respect all participants are exposed to the identical program however implementation of teaching techniques presented at the workshop will most likely vary from campus to campus based on factors such as the participantsâ€™ personal characteristics the differing demographics of the student bodies and differences in the university and departmental characteristics eg size of the student body organization of preservice courses department chairâ€™s support of the program goals departmental receptivity to change and innovation the qualitative analyst will need to discern patterns of interrelationships to suggest why the project promoted more change on some campuses than on others one technique for displaying narrative data is to develop a series of flow charts that map out any critical paths decision points and supporting evidence that emerge from establishing the data for a single site after the first flow chart has been developed the process can be repeated for all remaining sites analysts may 1 use the data from subsequent sites to modify the original flow chart 2 prepare an independent flow chart for each site andor 3 prepare a single flow chart for some events if most sites adopted a generic approach and multiple flow charts for others examination of the data display across the eight campuses might produce a finding that implementation proceeded more quickly and effectively on those campuses where the department chair was highly supportive of trying new approaches to teaching but was stymied and delayed when department chairs had misgivings about making changes to a triedandtrue system data display for intracase analysis exhibit 10 presents a data display matrix for analyzing patterns of response concerning perceptions and assessments of knowledgesharing activities for one campus we have assumed that three respondent units  participating faculty nonparticipating faculty and department chairs  have been asked similar questions looking at column a it is interesting that the three respondent groups were not in total agreement even on which activities they named only the participants considered email a means of sharing what they had learned in the program with their colleagues the nonparticipant colleagues apparently viewed the situation differently because they did not include email in their list the department chair  perhaps because she was unaware they were taking place  did not mention email or informal interchanges as knowledgesharing activities column b shows which activities each group considered most effective as a way of sharing knowledge in order of perceived importance column c summarizes the respondents reasons for regarding those particular activities as most effective looking down column b we can see that there is some overlap across groups  for example both the participants and the department chair believed structured seminars were the most effective knowledgesharing activity nonparticipants saw the structured seminars as better than lunchtime meetings but not as effective as informal interchanges exhibit 10 data matrix for campus a what was done to share knowledge respondent group aactivities named b which most effective cwhy participants structured seminars email informal interchanges lunchtime meetings structured seminars email concise way of communicating a lot of information nonparticipants structured seminars informal interchanges lunchtime meetings informal interchanges structured seminars easier to assimilate information in less formal settings smaller bits of information at a time department chair structured seminars lunch time meetings structured seminars highest attendance by nonparticipants most comments positive to chair simply knowing what each set of respondents considered most effective without knowing why would leave out an important piece of the analytic puzzle it would rob the qualitative analyst of the chance to probe potentially meaningful variations in underlying conceptions of what defines effectiveness in an educational exchange for example even though both participating faculty and the department chair agreed on the structured seminars as the most effective knowledgesharing activity they gave somewhat different reasons for making this claim the participants saw the seminars as the most effective way of communicating a lot of information concisely the department chair used indirect indicators  attendance rates of nonparticipants at the seminars as well as favorable comments on the seminars volunteered to her  to formulate her judgment of effectiveness it is important to recognize the different bases on which the respondents reached the same conclusions several points concerning qualitative analysis emerge from this relatively straightforward and preliminary exercise first a pattern of crossgroup differences can be discerned even before we analyze the responses concerning the activities regarded as most effective and why the openended format of the question allowed each group to give its own definition of knowledgesharing activities the point of the analysis is not primarily to determine which activities were used and how often if that were the major purpose of asking this question there would be far more efficient ways eg a checklist or rating scale to find the answer from an analytic perspective it is more important to begin to uncover relevant group differences in perceptions differences in reasons for considering one activity more effective than another might also point to different conceptions of the primary goals of the knowledgesharing activities some of these variations might be attributed to the fact that the respondent groups occupy different structural positions in life and different roles in this specific situation while both participating and nonparticipating faculty teach in the same department in this situation the participating faculty are playing a teaching role visavis their colleagues the data in column c indicate the participants see their main goal as imparting a great deal of information as concisely as possible by contrast the nonparticipants  in the role of students  believe they assimilate the material better when presented with smaller quantities of information in informal settings their different approaches to the question might reflect different perceptions based on this temporary rearrangement in their roles the department chair occupies a different structural position in the university than either the participating or nonparticipating faculty she may be too removed from daytoday exchanges among the faculty to see much of what is happening on this more informal level by the same token her removal from the grassroots might give her a broader perspective on the subject data display in crosscase analysis the principles applied in analyzing across cases essentially parallel those employed in the intracase analysis exhibit 11 shows an example of a hypothetical data display matrix that might be used for analysis of program participantsâ€™ responses to the knowledgesharing question across all eight campuses looking down column a one sees differences in the number and variety of knowledgesharing activities named by participating faculty at the eight schools brown bag lunches department newsletters workshops and dissemination of written hardcopy materials have been added to the list which for branch campus a included only structured seminars email informal interchanges and lunchtime meetings this expanded list probably encompasses most if not all such activities at the eight campuses in addition where applicable we have indicated whether the nonparticipating faculty involvement in the activity was compulsory or voluntary in exhibit 11 we are comparing the same group on different campuses rather than different groups on the same campus as in exhibit 10 column b reveals some overlap across participants in which activities were considered most effective structured seminars were named by participants at campuses a and c brown bag lunches exhibit 11 participantsâ€™ views of information sharing at eight campuses branch campus a activities named b which most effective cwhy astructured seminar voluntaryemail informal interchanges lunchtime meetings structured seminar email concise way of communicating a lot of information bbrown bags email department newsletter brown bags most interactive cworkshops voluntarystructured seminar compulsorystructured seminar compulsory structured format works well dinformal interchanges dissemination of written materials combination of the two dissemination important but not enough without personal touchestructured seminars compulsoryworkshops voluntaryworkshops voluntary handson approach works best femail dissemination of materials workshops compulsorydissemination of materials not everyone regularly uses email compulsory workshops resisted as coercive gstructured seminar informal interchanges lunch meetings lunch meetings best time hbrown bags email dissemination of materials brown bags relaxed environmentby those at campuses b and h however as in exhibit 10 the primary reasons for naming these activities were not always the same brown bag lunches were deemed most effective because of their interactive nature campus b and the relaxed environment in which they took place campus h both suggesting a preference for less formal learning situations however while campus a participants judged voluntary structured seminars the most effective way to communicate a great deal of information campus c participants also liked that the structured seminars on their campus were compulsory participants at both campuses appear to favor structure but may part company on whether requiring attendance is a good idea the voluntarycompulsory distinction was added to illustrate different aspects of effective knowledge sharing that might prove analytically relevant it would also be worthwhile to examine the reasons participants gave for deeming one activity more effective than another regardless of the activity data in column c show a tendency for participants on campuses b d e f and h to prefer voluntary informal handson personal approaches by contrast those from campuses a and c seemed to favor more structure although they may disagree on voluntary versus compulsory approaches the answer supplied for campus g best time is ambiguous and requires returning to the transcripts to see if more material can be found to clarify this response to have included all the knowledgesharing information from four different respondent groups on all eight campuses in a single matrix would have been quite complicated therefore for claritys sake we present only the participating faculty responses however to complete the crosscase analysis of this evaluation question the same procedure should be followed  if not in matrix format then conceptually  for nonparticipating faculty and department chairpersons for each group the analysis would be modeled on the above example it would be aimed at identifying important similarities and differences in what the respondents said or observed and exploring the possible bases for these patterns at different campuses much of qualitative analysis whether intracase or crosscase is structured by what glaser and strauss 1967 called the method of constant comparison an intellectually disciplined process of comparing and contrasting across instances to establish significant patterns then further questioning and refinement of these patterns as part of an ongoing analytic process conclusion drawing and verification this activity is the third element of qualitative analysis conclusion drawing involves stepping back to consider what the analyzed data mean and to assess their implications for the questions at hand 6 verification integrally linked to conclusion drawing entails revisiting the data as many times as necessary to crosscheck or verify these emergent conclusions the meanings emerging from the data have to be tested for their plausibility their sturdiness their â€˜confirmabilityâ€™  that is their validity miles and huberman 1994 p 11 validity means something different in this context than in quantitative evaluation where it is a technical term that refers quite specifically to whether a given construct measures what it purports to measure here validity encompasses a much broader concern for whether the conclusions being drawn from the data are credible defensible warranted and able to withstand alternative explanations6when qualitative data are used as a precursor to the designdevelopment of quantitative instruments this step may be postponed reducing the data and looking for relationships will provide adequate information for developing other instruments for many qualitative evaluators it is above all this third phase that gives qualitative analysis its special appeal at the same time it is probably also the facet that quantitative evaluators and others steeped in traditional quantitative techniques find most disquieting once qualitative analysts begin to move beyond cautious analysis of the factual data the critics ask what is to guarantee that they are not engaging in purely speculative flights of fancy indeed their concerns are not entirely unfounded if the unprocessed data heap is the result of not taking responsibility for shaping the story line of the analysis the opposite tendency is to take conclusion drawing well beyond what the data reasonably warrant or to prematurely leap to conclusions and draw implications without giving the data proper scrutiny the question about knowledge sharing provides a good example the underlying expectation or hope is for a diffusion effort wherein participating faculty stimulate innovation in teaching mathematics among their colleagues a crosscase finding might be that participating faculty at three of the eight campuses made active ongoing efforts to share their new knowledge with their colleagues in a variety of formal and informal settings at two other campuses initial efforts at sharing started strong but soon fizzled out and were not continued in the remaining three cases one or two faculty participants shared bits and pieces of what they had learned with a few selected colleagues on an ad hoc basis but otherwise took no steps to diffuse their new knowledge and skills more broadly taking these findings at face value might lead one to conclude that the project had largely failed in encouraging diffusion of new pedagogical knowledge and skills to nonparticipating faculty after all such sharing occurred in the desired fashion at only three of the eight campuses however before jumping ahead to conclude that the project was disappointing in this respect or to generalize beyond this case to other similar efforts at spreading pedagogic innovations among faculty it is vital to examine more closely the likely reasons why sharing among participating and nonparticipating faculty occurred and where and how it did the analysts would first look for factors distinguishing the three campuses where ongoing organized efforts at sharing did occur from those where such efforts were either not sustained or occurred in largely piecemeal fashion however it will also be important to differentiate among the less successful sites to tease out factors related both to the extent of sharing and the degree to which activities were sustained one possible hypothesis would be that successfully sustaining organized efforts at sharing on an ongoing basis requires structural supports at the departmental level andor conducive environmental conditions at the home campus in the absence of these supports a great burst of energy and enthusiasm at the beginning of the academic year will quickly give way under the pressure of the myriad demands as happened for the second group of two campuses similarly under most circumstances the individual good will of one or two participating faculty on a campus will in itself be insufficient to generate the type and level of exchange that would make a difference to the nonparticipating faculty the third set of campuses at the three successful sites for example faculty schedules may allow regularly scheduled common periods for colleagues to share ideas and information in addition participation in such events might be encouraged by the department chair and possibly even considered as a factor in making promotion and tenure decisions the department might also contribute a few dollars for refreshments in order to promote a more informal relaxed atmosphere at these activities in other words at the campuses where sharing occurred as desired conditions were conducive in one or more ways a new time slot did not have to be carved out of already crowded faculty schedules the department chair did more than simply pay lip service to the importance of sharing faculty are usually quite astute at picking up on what really matters in departmental culture and efforts were made to create a relaxed ambiance for transfer of knowledge at some of the other campuses structural conditions might not be conducive in that classes are taught continuously from 8 am through 8 pm with faculty coming and going at different times and on alternating days at another campus scheduling might not present so great a hurdle however the department chair may be so busy that despite philosophic agreement with the importance of diffusing the newly learned skills she can do little to actively encourage sharing among participating and nonparticipating faculty in this case it is not structural conditions or lukewarm support so much as competing priorities and the department chairs failure to act concretely on her commitment that stood in the way by contrast at another campus the department chairperson may publicly acknowledge the goals of the project but really believe it a waste of time and resources his failure to support sharing activities among his faculty stems from more deeply rooted misgivings about the value and viability of the project this distinction might not seem to matter given that the outcome was the same on both campuses sharing did not occur as desired however from the perspective of an evaluation researcher whether the department chair believes in the project could make a major difference to what would have to be done to change the outcome we have begun to develop a reasonably coherent explanation for the crosssite variations in the degree and nature of sharing taking place between participating and nonparticipating faculty arriving at this point required stepping back and systematically examining and reexamining the data using a variety of what miles and huberman 1994 pp 245262 call tactics for generating meaning they describe 13 such tactics including noting patterns and themes clustering cases making contrasts and comparisons partitioning variables and subsuming particulars in the general qualitative analysts typically employ some or all of these simultaneously and iteratively in drawing conclusions one factor that can impede conclusion drawing in evaluation studies is that the theoretical or logical assumptions underlying the research are often left unstated in this example as discussed above these are assumptions or expectations about knowledge sharing and diffusion of innovative practices from participating to nonparticipating faculty and by extension to their students for the analyst to be in a position to take advantage of conclusiondrawing opportunities he or she must be able to recognize and address these assumptions which are often only implicit in the evaluation questions toward that end it may be helpful to explicitly spell out a logic model or set of assumptions as to how the program is expected to achieve its desired outcome s recognizing these assumptions becomes even more important when there is a need or desire to place the findings from a single evaluation into wider comparative context visavis other program evaluations once having created an apparently credible explanation for variations in the extent and kind of sharing that occurs between participating and nonparticipating faculty across the eight campuses how can the analyst verify the validity  or truth value  of this interpretation of the data miles and huberman 1994 pp 262277 outline 13 tactics for testing or confirming findings all of which address the need to build systematic safeguards against selfdelusion p 265 into the process of analysis we will discuss only a few of these which have particular relevance for the example at hand and emphasize critical contrasts between quantitative and qualitative analytic approaches however two points are very important to stress at the outset several of the most important safeguards on validity  such as using multiple sources and modes of evidence  must be built into the design from the beginning and the analytic objective is to create a plausible empirically grounded account that is maximally responsive to the evaluation questions at hand as the authors note you are not looking for one account forsaking all others but for the best of several alternative accounts p 274 one issue of analytic validity that often arises concerns the need to weigh evidence drawn from multiple sources and based on different data collection modes such as selfreported interview responses and observational data triangulation of data sources and modes is critical but the results may not necessarily corroborate one another and may even conflict for example another of the summative evaluation questions proposed in chapter 2 concerns the extent to which nonparticipating faculty adopt new concepts and practices in their teaching answering this question relies on a combination of observations selfreported data from participant focus groups and indepth interviews with department chairs and nonparticipating faculty in this case there is a possibility that the observational data might be at odds with the selfreported data from one or more of the respondent groups for example when interviewed the vast majority of nonparticipating faculty might say and really believe that they are applying projectrelated innovative principles in their teaching however the observers may see very little behavioral evidence that these principles are actually influencing teaching practices in these faculty members classrooms it would be easy to brush off this finding by concluding that the nonparticipants are saving face by parroting what they believe they are expected to say about their teaching but there are other more analytically interesting possibilities perhaps the nonparticipants have an incomplete understanding of these principles or they were not adequately trained in how to translate them effectively into classroom practice the important point is that analyzing across multiple group perspectives and different types of data is not a simple matter of deciding who is right or which data are most accurate weighing the evidence is a more subtle and delicate matter of hearing each groups viewpoint while still recognizing that any single perspective is partial and relative to the respondents experiences and social position moreover as noted above respondents perceptions are no more or less real than observations in fact discrepancies between selfreported and observational data may reveal profitable topics or areas for further analysis it is the analysts job to weave the various voices and sources together in a narrative that responds to the relevant evaluation question s the more artfully this is done the simpler more natural it appears to the reader to go to the trouble to collect various types of data and listen to different voices only to pound the information into a flattened picture is to do a real disservice to qualitative analysis however if there is a reason to believe that some of the data are stronger than others some of the respondents are highly knowledgeable on the subject while others are not it is appropriate to give these responses greater weight in the analysis qualitative analysts should also be alert to patterns of interconnection in their data that differ from what might have been expected miles and huberman define these as following up surprises 1994 p 270 for instance at one campus systematically comparing participating and nonparticipating faculty responses to the question about knowledgesharing activities see exhibit 10 might reveal few apparent crossgroup differences however closer examination of the two sets of transcripts might show meaningful differences in perceptions dividing along other less expected lines for purposes of this evaluation it was tacitly assumed that the relevant distinctions between faculty would most likely be between those who had and had not participated in the project however both groups also share a history as faculty in the same department therefore other factors  such as prior personal ties  might have overridden the participantnonparticipant faculty distinction one strength of qualitative analysis is its potential to discover and manipulate these kinds of unexpected patterns which can often be very informative to do this requires an ability to listen for and be receptive to surprises unlike quantitative researchers who need to explain away deviant or exceptional cases qualitative analysts are also usually delighted when they encounter twists in their data that present fresh analytic insights or challenges miles and huberman 1994 pp 269 270 talk about checking the meaning of outliers and using extreme cases in qualitative analysis deviant instances or cases that do not appear to fit the pattern or trend are not treated as outliers as they would be in statistical probabilitybased analysis rather deviant or exceptional cases should be taken as a challenge to further elaboration and verification of an evolving conclusion for example if the department chair strongly supports the projects aims and goals for all successful projects but one perhaps another set of factors is fulfilling the same function s at the deviant site identifying those factors will in turn help to clarify more precisely what it is about strong leadership and belief in a project that makes a difference or to elaborate on another extended example suppose at one campus where structural conditions are not conducive to sharing between participating and nonparticipating faculty such sharing is occurring nonetheless spearheaded by one very committed participating faculty member this example might suggest that a highly committed individual who is a natural leader among his faculty peers is able to overcome the structural constraints to sharing in a sense this deviant case analysis would strengthen the general conclusion by showing that it takes exceptional circumstances to override the constraints of the situation elsewhere in this handbook we noted that summative and formative evaluations are often linked by the premise that variations in project implementation will in turn effect differences in project outcomes in the hypothetical example presented in this handbook all participants were exposed to the same activities on the central campus eliminating the possibility of analyzing the effects of differences in implementation features however using a different model and comparing implementation and outcomes at three different universities with three campuses participating per university would give some idea of what such an analysis might look like a display matrix for a crosssite evaluation of this type is given in exhibit 12 the upper portion of the matrix shows how the three campuses varied in key implementation features the bottom portion summarizes outcomes at each campus while we would not necessarily expect a onetoone relationship the matrix loosely pairs implementation features with outcomes with which they might be associated for example workshop staffing and delivery are paired with knowledgesharing activities accuracy of workshop content with curricular change however there is nothing to preclude looking for a relationship between use of appropriate techniques in the workshops formative and curricular changes on the campuses summative use of the matrix would essentially guide the analysis along the same lines as in the examples provided earlier exhibit 12 matrix of crosscase analysis linking implementation andoutcome factors implementation features branch campus workshops delivered and staffed as planned content accurate up to date appropriate techniques used materials available suitable presentation campus a yes yes for most participants yes but delayed mostly campus b no yes yes no very mixed reviews campus c mostly yes for a few participants yes some outcome features  participating campuses branch campus knowledge sharing with nonparticipants curricular changes changes to exams and requirements expenditures students more interested active in class campus a high level many some no some campuses campus b low level many many yes mostly participants students campus c moderate level only a few few yes only minor improvement in this crosssite analysis the overarching question would address the similarities and differences across these three sites  in terms of project implementation outcomes and the connection between them  and investigate the bases of these differences was one of the projects discernibly more successful than others either overall or in particular areas  and if so what factors or configurations of factors seem to have contributed to these successes the analysis would then continue through multiple iterations until a satisfactory resolution is achieved summary judging the quality of qualitative analysis issues surrounding the value and uses of conclusion drawing and verification in qualitative analysis take us back to larger questions raised at the outset about how to judge the validity and quality of qualitative research a lively debate rages on these and related issues it goes beyond the scope of this chapter to enter this discussion in any depth but it is worthwhile to summarize emerging areas of agreement first although stated in different ways there is broad consensus concerning the qualitative analysts need to be selfaware honest and reflective about the analytic process analysis is not just the end product it is also the repertoire of processes used to arrive at that particular place in qualitative analysis it is not necessary or even desirable that anyone else who did a similar study should find exactly the same thing or interpret his or her findings in precisely the same way however once the notion of analysis as a set of uniform impersonal universally applicable procedures is set aside qualitative analysts are obliged to describe and discuss how they did their work in ways that are at the very least accessible to other researchers open and honest presentation of analytic processes provides an important check on an individual analystâ€™s tendencies to get carried away allowing others to judge for themselves whether the analysis and interpretation are credible in light of the data second qualitative analysis as all of qualitative research is in some ways craftsmanship kvale 1995 there is such a thing as poorly crafted or bad qualitative analysis and despite their reluctance to issue universal criteria seasoned qualitative researchers of different bents can still usually agree when they see an example of it analysts should be judged partly in terms of how skillfully artfully and persuasively they craft an argument or tell a story does the analysis flow well and make sense in relation to the studys objectives and the data that were presented is the story line clear and convincing is the analysis interesting informative provocative does the analyst explain how and why she or he drew certain conclusions or on what bases she or he excluded other possible interpretations these are the kinds of questions that can and should be asked in judging the quality of qualitative analyses in evaluation studies analysts are often called upon to move from conclusions to recommendations for improving programs and policies the recommendations should fit with the findings and with the analystsâ€™ understanding of the context or milieu of the study it is often useful to bring in stakeholders at the point of translating analytic conclusions to implications for action as should by now be obvious it is truly a mistake to imagine that qualitative analysis is easy or can be done by untrained novices as patton 1990 comments applying guidelines requires judgment and creativity because each qualitative study is unique the analytical approach used will be unique because qualitative inquiry depends at every stage on the skills training insights and capabilities of the researcher qualitative analysis ultimately depends on the analytical intellect and style of the analyst the human factor is the greatest strength and the fundamental weakness of qualitative inquiry and analysis practical advice in conducting qualitative analyses start the analysis right away and keep a running account of it in your notes it cannot be overstressed that analysis should begin almost in tandem with data collection and that it is an iterative set of processes that continues over the course of the field work and beyond it is generally helpful for field notes or focus group or interview summaries to include a section containing comments tentative interpretations or emerging hypotheses these may eventually be overturned or rejected and will almost certainly be refined as more data are collected but they provide an important account of the unfolding analysis and the internal dialogue that accompanied the process involve more than one person two heads are better than one and three may be better still qualitative analysis need not and in many cases probably should not be a solitary process it is wise to bring more than one person into the analytic process to serve as a crosscheck sounding board and source of new ideas and crossfertilization it is best if all analysts know something about qualitative analysis as well as the substantive issues involved if it is impossible or impractical for a second or third person to play a central role his or her skills may still be tapped in a more limited way for instance someone might review only certain portions of a set of transcripts leave enough time and money for analysis and writing analyzing and writing up qualitative data almost always takes more time thought and effort than anticipated a budget that assumes a week of analysis time and a week of writing for a project that takes a yearâ€™s worth of field work is highly unrealistic along with revealing a lack of understanding of the nature of qualitative analysis failing to build in enough time and money to complete this process adequately is probably the major reason why evaluation reports that include qualitative data can disappoint be selective when using computer software packages in qualitative analysis a great proliferation of software packages that can be used to aid analysis of qualitative data has been developed in recent years most of these packages were reviewed by weitzman and miles 1995 who grouped them into six types word processors word retrievers textbase managers codeandretrieve programs codebased theory builders and conceptual network builders all have strengths and weaknesses weitzman and miles suggested that when selecting a given package researchers should think about the amount types and sources of data to be analyzed and the types of analyses that will be performed two caveats are in order first computer software packages for qualitative data analysis essentially aid in the manipulation of relevant segments of text while helpful in marking coding and moving data segments more quickly and efficiently than can be done manually the software cannot determine meaningful categories for coding and analysis or define salient themes or factors in qualitative analysis as seen above concepts must take precedence over mechanics the analytic underpinnings of the procedures must still be supplied by the analyst software packages cannot and should not be used as a way of evading the hard intellectual labor of qualitative analysis second since it takes time and resources to become adept in utilizing a given software package and learning its peculiarities researchers may want to consider whether the scope of their project or their ongoing needs truly warrant the investment references berkowitz s 1996 using qualitative and mixed method approaches chapter 4 in needs assessment a creative and practical guide for social scientists r reviere s berkowitz c c carter and c gravesferguson eds washington dc taylor  francis glaser b and strauss a 1967 the discovery of grounded theory chicago aldine kvale s 1995 the social construction of validity qualitative inquiry 11940 miles m b and huberman a m 1984 qualitative data analysis 16 newbury park ca sage miles m b and huberman a m 1994 qualitative data analysis 2nd ed p 1012 newbury park ca sage patton m q 1990 qualitative evaluation and research methods 2nd ed newbury park ca sage weitzman e a and miles m b 1995 a software sourcebook computer programs for qualitative data analysis thousand oaks ca sage other recommended reading coffey a and atkinson p 1996 making sense of qualitative data complementary research strategies thousand oaks ca sage howe k and eisenhart m 1990 standards for qualitative and quantitative research a prolegomenon educational researcher 19 429 wolcott h f 1994 t ransforming qualitative data description analysis and interpretation thousand oaks ca sage previous chapter  back to top  next chapter table of contents 